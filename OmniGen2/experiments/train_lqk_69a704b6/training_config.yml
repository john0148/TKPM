cache_dir: null
data:
  data_path: /mnt/w/TKPM/backend/training_data/69a704b6-d404-4a21-8135-b217bee55e53/dataset/train_config.yml
  max_input_pixels:
  - 524288
  - 524288
  - 262144
  - 131072
  max_output_pixels: 524288
  max_side_length: 2048
  maximum_text_tokens: 888
  prompt_dropout_prob: 0.0001
  ref_img_dropout_prob: 0.1
  use_chat_template: true
device_specific_seed: true
logger:
  checkpointing_steps: 200
  checkpoints_total_limit: 3
  log_with: []
model:
  arch_opt:
    axes_dim_rope:
    - 40
    - 40
    - 40
    axes_lens:
    - 10000
    - 10000
    - 10000
    hidden_size: 2520
    in_channels: 16
    multiple_of: 256
    norm_eps: 1.0e-05
    num_attention_heads: 21
    num_kv_heads: 7
    num_layers: 32
    num_refiner_layers: 2
    patch_size: 2
    text_feat_dim: 2048
    timestep_scale: 1000
  pretrained_model_path: OmniGen2/OmniGen2
  pretrained_text_encoder_model_name_or_path: Qwen/Qwen2.5-VL-3B-Instruct
  pretrained_vae_model_name_or_path: black-forest-labs/FLUX.1-dev
name: train_lqk_69a704b6
resume_from_checkpoint: null
seed: 2233
train:
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-08
  adam_weight_decay: 0.01
  allow_tf32: false
  batch_size: 1
  cpu_offload_optimizer_state: true
  cpu_offload_params: true
  dataloader_num_workers: 1
  dataloader_pin_memory: false
  deepspeed: null
  ema_decay: 0.0
  fsdp: null
  global_batch_size: 4
  gradient_accumulation_steps: 2
  gradient_checkpointing: true
  learning_rate: 8.0e-07
  lora_dropout: 0
  lora_ft: true
  lora_rank: 8
  lr_scheduler: timm_constant_with_warmup
  max_grad_norm: 1
  max_train_steps: 1000
  mixed_precision: bf16
  scale_lr: false
  set_grads_to_none: true
  t_in_epochs: false
  use_8bit_adam: true
  warmup_lr_init: 1.0e-18
  warmup_prefix: true
  warmup_t: 100
transport:
  do_shift: true
  dynamic_time_shift: true
  snr_type: lognorm
val:
  train_visualization_steps: 50
  validation_steps: 200
workder_specific_seed: true
